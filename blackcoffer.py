# -*- coding: utf-8 -*-
"""Blackcoffer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ch8kvEYywerV84AaCfuyWXVtuGavXbP0
"""

import os
print(os.getcwd())

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

# Read the input Excel file
input_file = input_file = "/content/Input.xlsx"

input_df = pd.read_excel(input_file)

# Function to extract article text from a given URL
def extract_article_text(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad responses

        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract article title and text
        article_title = soup.title.text.strip()
        article_text = ''
        article_text_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        for tag in article_text_tags:
            article_text += tag.text.strip() + '\n'

        return article_title, article_text
    except Exception as e:
        print(f"Error extracting article from {url}: {str(e)}")
        return None, None

# Create a directory to save the extracted text files
output_directory = 'extracted_text'
os.makedirs(output_directory, exist_ok=True)

# Iterate through each URL in the input DataFrame
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Extract article text
    article_title, article_text = extract_article_text(url)

    if article_title and article_text:
        # Save the extracted text to a text file
        output_file = os.path.join(output_directory, f'{url_id}.txt')
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(f"{article_title}\n\n{article_text}")

        print(f"Text extracted from {url_id} and saved to {output_file}")

print("Extraction and saving process completed.")

import os
import nltk
import pandas as pd
from nltk import FreqDist
from nltk.tokenize import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Function to perform text analysis on the given text
def text_analysis(text):
    # Tokenization
    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]

    # Frequency distribution
    freq_dist = FreqDist(filtered_tokens)

    # Sentiment analysis
    sid = SentimentIntensityAnalyzer()
    sentiment_score = sid.polarity_scores(text)

    return freq_dist, sentiment_score

# Folder containing extracted text files
extracted_text_folder = 'extracted_text'

# Create a DataFrame to store the analysis results
columns = ['File', 'Top 10 Words', 'Sentiment Score']
results_df = pd.DataFrame(columns=columns)

# Loop through each text file and perform text analysis
for filename in os.listdir(extracted_text_folder):
    file_path = os.path.join(extracted_text_folder, filename)

    # Read the content of the text file
    with open(file_path, 'r', encoding='utf-8') as file:
        article_text = file.read()

    # Perform text analysis
    freq_dist, sentiment_score = text_analysis(article_text)

    # Save results to DataFrame
    results_df = results_df.append({
        'File': filename,
        'Top 10 Words': freq_dist.most_common(10),
        'Sentiment Score': sentiment_score['compound']
    }, ignore_index=True)

# Save the results to an Excel file
output_excel_path = 'text_analysis_results.xlsx'
results_df.to_excel(output_excel_path, index=False)

print(f"Text analysis results saved to {output_excel_path}")

import os
import nltk
import pandas as pd
import textstat
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Function to perform extended text analysis on the given text
def extended_text_analysis(text):
    # Tokenization
    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]

    # Sentiment analysis
    sid = SentimentIntensityAnalyzer()
    sentiment_score = sid.polarity_scores(text)

    # Calculate additional metrics using textstat
    avg_sentence_length = textstat.avg_sentence_length(text)
    fog_index = textstat.gunning_fog(text)
    avg_words_per_sentence = textstat.avg_sentence_length(text)
    difficult_words_count = textstat.difficult_words(text)
    word_count = textstat.lexicon_count(text)
    syllables_per_word = textstat.syllable_count(text) / max(1, word_count)  # Avoid division by zero
    personal_pronouns = sum(1 for word in tokens if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])
    avg_word_length = sum(len(word) for word in filtered_tokens) / max(1, len(filtered_tokens))  # Avoid division by zero

    # Calculate percentage of complex words
    percentage_complex_words = (difficult_words_count / max(1, word_count)) * 100

    return {
        'Positive Score': sentiment_score['pos'],
        'Negative Score': sentiment_score['neg'],
        'Polarity Score': sentiment_score['compound'],
        'Subjectivity Score': sentiment_score['neu'],
        'Avg Sentence Length': avg_sentence_length,
        'Percentage of Complex Words': percentage_complex_words,
        'FOG Index': fog_index,
        'Avg Number of Words per Sentence': avg_words_per_sentence,
        'Complex Word Count': difficult_words_count,
        'Word Count': word_count,
        'Syllables per Word': syllables_per_word,
        'Personal Pronouns': personal_pronouns,
        'Avg Word Length': avg_word_length
    }

# Folder containing extracted text files
extracted_text_folder = 'extracted_text'

# Create a DataFrame to store the extended analysis results
columns = ['File'] + list(extended_text_analysis(""))  # Initialize with keys from an example
results_df = pd.DataFrame(columns=columns)

# Loop through each text file and perform extended text analysis
for filename in os.listdir(extracted_text_folder):
    file_path = os.path.join(extracted_text_folder, filename)

    # Read the content of the text file
    with open(file_path, 'r', encoding='utf-8') as file:
        article_text = file.read()

    # Perform extended text analysis
    analysis_results = extended_text_analysis(article_text)

    # Save results to DataFrame
    results_df = results_df.append({'File': filename, **analysis_results}, ignore_index=True)

# Save the extended results to an Excel file
output_excel_path = 'extended_text_analysis_results.xlsx'
results_df.to_excel(output_excel_path, index=False)

print(f"Extended text analysis results saved to {output_excel_path}")

